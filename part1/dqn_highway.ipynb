{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import gymnasium as gym\n",
    "\n",
    "import os\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import time\n",
    "\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "\n",
    "\n",
    "def eval_agent(agent, env, n_sim=5):\n",
    "    \"\"\"\n",
    "    ** Solution **\n",
    "\n",
    "    Monte Carlo evaluation of DQN agent.\n",
    "\n",
    "    Repeat n_sim times:\n",
    "        * Run the DQN policy until the environment reaches a terminal state (= one episode)\n",
    "        * Compute the sum of rewards in this episode\n",
    "        * Store the sum of rewards in the episode_rewards array.\n",
    "    \"\"\"\n",
    "    env_copy = deepcopy(env)\n",
    "    episode_rewards = np.zeros(n_sim)\n",
    "    for i in range(n_sim):\n",
    "        state, _ = env_copy.reset()\n",
    "        reward_sum = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.get_action(state, 0)\n",
    "            state, reward, terminated, truncated, _ = env_copy.step(action)\n",
    "            reward_sum += reward\n",
    "            done = terminated or truncated\n",
    "        episode_rewards[i] = reward_sum\n",
    "    return episode_rewards\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, terminated, next_state):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = (state, action, reward, terminated, next_state)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.choices(self.memory, k=batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic neural net.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # return self.net(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        action_space,\n",
    "        observation_space,\n",
    "        gamma,\n",
    "        batch_size,\n",
    "        buffer_capacity,\n",
    "        update_target_every,\n",
    "        epsilon_start,\n",
    "        decrease_epsilon_factor,\n",
    "        epsilon_min,\n",
    "        learning_rate,\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.update_target_every = update_target_every\n",
    "\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.decrease_epsilon_factor = (\n",
    "            decrease_epsilon_factor  # larger -> more exploration\n",
    "        )\n",
    "        self.epsilon_min = epsilon_min\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def update(self, state, action, reward, terminated, next_state):\n",
    "        \"\"\"\n",
    "        ** SOLUTION **\n",
    "        \"\"\"\n",
    "        # Convert numpy arrays or lists to tensors and ensure they are floats\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "        action_tensor = torch.tensor([[action]], dtype=torch.int64)\n",
    "        reward_tensor = torch.tensor([reward], dtype=torch.float32)\n",
    "        terminated_tensor = torch.tensor([terminated], dtype=torch.float32)\n",
    "\n",
    "        # Store transition in the replay buffer\n",
    "        self.buffer.push(\n",
    "            state_tensor,\n",
    "            action_tensor,\n",
    "            reward_tensor,\n",
    "            terminated_tensor,\n",
    "            next_state_tensor,\n",
    "        )\n",
    "\n",
    "        # # add data to replay buffer\n",
    "        # self.buffer.push(\n",
    "        #     torch.tensor(state).unsqueeze(0),\n",
    "        #     torch.tensor([[action]], dtype=torch.int64),\n",
    "        #     torch.tensor([reward]),\n",
    "        #     torch.tensor([terminated], dtype=torch.int64),\n",
    "        #     torch.tensor(next_state, dtype=torch.float).unsqueeze(0),\n",
    "        # )\n",
    "\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return np.inf\n",
    "\n",
    "        # get batch\n",
    "        transitions = self.buffer.sample(self.batch_size)\n",
    "\n",
    "        state_batch, action_batch, reward_batch, terminated_batch, next_state_batch = (\n",
    "            tuple([torch.cat(data) for data in zip(*transitions)])\n",
    "        )\n",
    "\n",
    "        values = self.q_net.forward(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute the ideal Q values\n",
    "        with torch.no_grad():\n",
    "            next_state_values = (1 - terminated_batch) * self.target_net(\n",
    "                next_state_batch\n",
    "            ).max(1)[0]\n",
    "            targets = next_state_values * self.gamma + reward_batch\n",
    "\n",
    "        loss = self.loss_function(values, targets.unsqueeze(1))\n",
    "\n",
    "        # Optimize the model with gradient clipping\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), 1)  # Gradient clipping\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Soft update the target network\n",
    "        for target_param, local_param in zip(\n",
    "            self.target_net.parameters(), self.q_net.parameters()\n",
    "        ):\n",
    "            target_param.data.copy_(\n",
    "                0.995 * target_param.data + 0.005 * local_param.data\n",
    "            )\n",
    "\n",
    "        self.scheduler.step()  # Step through the scheduler\n",
    "\n",
    "        if not ((self.n_steps + 1) % self.update_target_every):\n",
    "            self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "        self.decrease_epsilon()\n",
    "\n",
    "        self.n_steps += 1\n",
    "        if terminated:\n",
    "            self.n_eps += 1\n",
    "\n",
    "        return loss.detach().numpy()\n",
    "\n",
    "    def get_action(self, state, epsilon=None):\n",
    "        \"\"\"\n",
    "        Return action according to an epsilon-greedy exploration policy\n",
    "        \"\"\"\n",
    "        if epsilon is None:\n",
    "            epsilon = self.epsilon\n",
    "\n",
    "        if np.random.rand() < epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.get_q(state))\n",
    "\n",
    "    def get_q(self, state):\n",
    "        \"\"\"\n",
    "        Compute Q function for a states\n",
    "        \"\"\"\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            output = self.q_net.forward(state_tensor)  # shape (1,  n_actions)\n",
    "        return output.numpy()[0]  # shape  (n_actions)\n",
    "\n",
    "    def decrease_epsilon(self):\n",
    "        self.epsilon = self.epsilon_min + (self.epsilon_start - self.epsilon_min) * (\n",
    "            np.exp(-1.0 * self.n_eps / self.decrease_epsilon_factor)\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        hidden_size = 256\n",
    "\n",
    "        # obs_size = self.observation_space.shape[0]\n",
    "        obs_size = np.prod(self.env.observation_space.shape)\n",
    "        n_actions = self.action_space.n\n",
    "\n",
    "        self.buffer = ReplayBuffer(self.buffer_capacity)\n",
    "        self.q_net = Net(obs_size, hidden_size, n_actions)\n",
    "        self.target_net = Net(obs_size, hidden_size, n_actions)\n",
    "        self.target_net.load_state_dict(\n",
    "            self.q_net.state_dict()\n",
    "        )  # Initialize target net\n",
    "        self.target_net.eval()  # Set target net to eval mode\n",
    "\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(\n",
    "            params=self.q_net.parameters(), lr=self.learning_rate, weight_decay=1e-5\n",
    "        )\n",
    "        self.scheduler = StepLR(\n",
    "            self.optimizer, step_size=100, gamma=0.99\n",
    "        )  # Learning rate scheduler\n",
    "\n",
    "        self.epsilon = self.epsilon_start\n",
    "        self.n_steps = 0\n",
    "        self.n_eps = 0\n",
    "\n",
    "\n",
    "def run_one_episode(env, agent, display=True):\n",
    "    display_env = deepcopy(env)\n",
    "    done = False\n",
    "    state, _ = display_env.reset()\n",
    "\n",
    "    rewards = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.get_action(state, 0)\n",
    "        state, reward, done, _, _ = display_env.step(action)\n",
    "        rewards += reward\n",
    "        if display:\n",
    "            clear_output(wait=True)\n",
    "            plt.imshow(display_env.render())\n",
    "            plt.show()\n",
    "    if display:\n",
    "        display_env.close()\n",
    "    print(f\"Episode length {rewards}\")\n",
    "\n",
    "\n",
    "def run_episodes_for_a_minute(env, agent, display=True):\n",
    "    start_time = time.time()\n",
    "    episodes = 0\n",
    "    total_rewards = 0\n",
    "\n",
    "    while time.time() - start_time < 10:  # Exécuter pendant environ une minute\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        episode_rewards = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.get_action(state, 0)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            episode_rewards += reward\n",
    "            if display:\n",
    "                clear_output(wait=True)\n",
    "                plt.imshow(env.render())\n",
    "                plt.show()\n",
    "\n",
    "        episodes += 1\n",
    "        total_rewards += episode_rewards\n",
    "        print(f\"Episode {episodes} reward: {episode_rewards}\")\n",
    "\n",
    "    print(f\"Nombre total d'épisodes exécutés en 1 minute : {episodes}\")\n",
    "    print(f\"Récompense moyenne sur les épisodes : {total_rewards / episodes}\")\n",
    "\n",
    "\n",
    "# env = env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "# agent = RandomAgent(env.observation_space, env.action_space)\n",
    "\n",
    "# # Exécuter la fonction pour des épisodes répétés pendant environ une minute\n",
    "# run_episodes_for_a_minute(env, agent)\n",
    "\n",
    "# # run_one_episode(env, agent, display=True)\n",
    "# print(f\"Average over 5 runs : {np.mean(eval_agent(agent, env))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, N_episodes, eval_every=10, reward_threshold=300):\n",
    "    total_time = 0\n",
    "    state, _ = env.reset()\n",
    "    losses = []\n",
    "    for ep in range(N_episodes):\n",
    "        done = False\n",
    "        state, _ = env.reset()\n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            loss_val = agent.update(state, action, reward, terminated, next_state)\n",
    "\n",
    "            state = next_state\n",
    "            losses.append(loss_val)\n",
    "\n",
    "            done = terminated or truncated\n",
    "            total_time += 1\n",
    "\n",
    "        if (ep + 1) % eval_every == 0:\n",
    "            rewards = eval_agent(agent, env)\n",
    "            print(\"episode =\", ep + 1, \", reward = \", np.mean(rewards))\n",
    "            if np.mean(rewards) >= reward_threshold:\n",
    "                break\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode = 10 , reward =  8.173862958010744\n",
      "episode = 20 , reward =  7.361362958010744\n",
      "episode = 30 , reward =  3.627059666168423\n",
      "episode = 40 , reward =  9.23125\n",
      "episode = 50 , reward =  11.2405\n",
      "episode = 60 , reward =  12.08125\n",
      "episode = 70 , reward =  10.197325398871403\n",
      "episode = 80 , reward =  4.9375\n",
      "episode = 90 , reward =  13.04375\n",
      "episode = 100 , reward =  12.375\n",
      "episode = 110 , reward =  2.65625\n",
      "episode = 120 , reward =  14.93125\n",
      "episode = 130 , reward =  11.61875\n",
      "episode = 140 , reward =  3.5973253988714027\n",
      "episode = 150 , reward =  5.381326687458437\n",
      "episode = 160 , reward =  8.04375\n",
      "episode = 170 , reward =  14.1875\n",
      "episode = 180 , reward =  10.56012549102264\n",
      "episode = 190 , reward =  3.6072644273558794\n",
      "episode = 200 , reward =  3.3563383069448514\n",
      "episode = 210 , reward =  2.953575398871403\n",
      "episode = 220 , reward =  11.63125\n",
      "episode = 230 , reward =  13.0125\n",
      "episode = 240 , reward =  2.8223253988714028\n",
      "episode = 250 , reward =  6.032228362218106\n",
      "episode = 260 , reward =  3.96\n",
      "episode = 270 , reward =  2.8277629080734483\n",
      "episode = 280 , reward =  7.128575398871402\n",
      "episode = 290 , reward =  5.85\n",
      "episode = 300 , reward =  8.84375\n",
      "episode = 310 , reward =  2.8098253988714026\n",
      "episode = 320 , reward =  3.9556515982943083\n",
      "episode = 330 , reward =  3.7169813384297625\n",
      "episode = 340 , reward =  5.011901598294308\n",
      "episode = 350 , reward =  1.9125\n",
      "episode = 360 , reward =  8.90625\n",
      "episode = 370 , reward =  4.0321559395583595\n",
      "episode = 380 , reward =  2.5104190936364716\n",
      "episode = 390 , reward =  1.875\n",
      "episode = 400 , reward =  2.35625\n",
      "episode = 410 , reward =  3.3946559395583598\n",
      "episode = 420 , reward =  2.646726997165711\n",
      "episode = 430 , reward =  2.52240593955836\n",
      "episode = 440 , reward =  2.821414589342124\n",
      "episode = 450 , reward =  2.475\n",
      "episode = 460 , reward =  4.44375\n",
      "episode = 470 , reward =  2.7125\n",
      "episode = 480 , reward =  3.3630936947650683\n",
      "episode = 490 , reward =  2.4928764730679185\n",
      "episode = 500 , reward =  2.4991264730679186\n",
      "episode = 510 , reward =  2.575\n",
      "episode = 520 , reward =  2.2306515982943083\n",
      "episode = 530 , reward =  2.181552396037114\n",
      "episode = 540 , reward =  7.153258151851301\n",
      "episode = 550 , reward =  5.509800101012334\n",
      "episode = 560 , reward =  2.552664589342124\n",
      "episode = 570 , reward =  5.4913829367240705\n",
      "episode = 580 , reward =  2.436901598294308\n",
      "episode = 590 , reward =  1.9556515982943083\n",
      "episode = 600 , reward =  5.7038754910226395\n",
      "episode = 610 , reward =  3.021398555730381\n",
      "episode = 620 , reward =  2.2624899882135265\n",
      "episode = 630 , reward =  1.7931515982943083\n",
      "episode = 640 , reward =  2.0806515982943083\n",
      "episode = 650 , reward =  2.1175531965886165\n",
      "episode = 660 , reward =  4.50625\n",
      "episode = 670 , reward =  6.003575398871403\n",
      "episode = 680 , reward =  5.684825398871403\n",
      "episode = 690 , reward =  5.931213059813784\n",
      "episode = 700 , reward =  2.297325398871403\n",
      "episode = 710 , reward =  3.25625\n",
      "episode = 720 , reward =  2.75\n",
      "episode = 730 , reward =  3.3375\n",
      "episode = 740 , reward =  3.33125\n",
      "episode = 750 , reward =  3.459226997165711\n",
      "episode = 760 , reward =  10.88125\n",
      "episode = 770 , reward =  6.175\n",
      "episode = 780 , reward =  1.9723253988714027\n",
      "episode = 790 , reward =  6.570497757987576\n",
      "episode = 800 , reward =  5.674981527632037\n",
      "episode = 810 , reward =  5.228117002977301\n",
      "episode = 820 , reward =  3.349350880143709\n",
      "episode = 830 , reward =  1.7056515982943083\n",
      "episode = 840 , reward =  1.6375\n",
      "episode = 850 , reward =  3.8\n",
      "episode = 860 , reward =  3.1586285954600193\n",
      "episode = 870 , reward =  2.1885\n",
      "episode = 880 , reward =  3.608914589342124\n",
      "episode = 890 , reward =  3.7634783622181063\n",
      "episode = 900 , reward =  3.41875\n",
      "episode = 910 , reward =  2.9175\n",
      "episode = 920 , reward =  3.04375\n",
      "episode = 930 , reward =  3.2375\n",
      "episode = 940 , reward =  5.24375\n",
      "episode = 950 , reward =  3.4556515982943083\n",
      "episode = 960 , reward =  3.366075398871403\n",
      "episode = 970 , reward =  5.945606968515088\n",
      "episode = 980 , reward =  2.485626997165711\n",
      "episode = 990 , reward =  3.2285753988714028\n",
      "episode = 1000 , reward =  2.596726997165711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2d2c4bdc0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Adjusted environment initialization for \"highway-fast-v0\"\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from config import config\n",
    "\n",
    "# Configure and create the environment\n",
    "env = gym.make(\"highway-fast-v0\", render_mode=\"rgb_array\", config=config)\n",
    "# env.unwrapped.configure(config)\n",
    "action_space = env.action_space\n",
    "observation_space = env.observation_space\n",
    "\n",
    "# Update the DQN Agent initialization with the new action and observation spaces\n",
    "# Note: Ensure that observation dimensions are correctly handled within your DQN architecture.\n",
    "# This might require adjustments depending on how the \"OccupancyGrid\" observations are structured.\n",
    "\n",
    "# Hyperparameters might need adjustment based on the new environment dynamics.\n",
    "gamma = 0.99\n",
    "batch_size = 128\n",
    "buffer_capacity = 20_000\n",
    "update_target_every = 32\n",
    "epsilon_start = 0.9\n",
    "decrease_epsilon_factor = 1500\n",
    "epsilon_min = 0.01\n",
    "learning_rate = 1e-4\n",
    "\n",
    "hidden_size = 256\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# When you instantiate the DQN agent:\n",
    "agent = DQN(\n",
    "    env,\n",
    "    action_space,\n",
    "    observation_space,\n",
    "    gamma,\n",
    "    batch_size,\n",
    "    buffer_capacity,\n",
    "    update_target_every,\n",
    "    epsilon_start,\n",
    "    decrease_epsilon_factor,\n",
    "    epsilon_min,\n",
    "    learning_rate,\n",
    ")\n",
    "\n",
    "# Training might need adjustments, especially evaluation metrics and thresholding for success.\n",
    "N_episodes = 1000\n",
    "\n",
    "# Proceed with the adjusted training function\n",
    "# Ensure that your training and evaluation routines properly handle the updated environment observations and actions.\n",
    "losses = train(env, agent, N_episodes)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean reward after training =  2.9354213331902965\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the final policy\n",
    "rewards = eval_agent(agent, env, 20)\n",
    "print(\"\")\n",
    "print(\"mean reward after training = \", np.mean(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zk/rcqnm5cj1s31mwy0myzr5df00000gp/T/ipykernel_43569/4237455485.py:215: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  state_tensor = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 7 at dim 1 (got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m         env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Visualize the agent\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mvisualize_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 12\u001b[0m, in \u001b[0;36mvisualize_agent\u001b[0;34m(env, agent, episodes)\u001b[0m\n\u001b[1;32m     10\u001b[0m img \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mimshow(env\u001b[38;5;241m.\u001b[39mrender())  \u001b[38;5;66;03m# only call this once\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m---> 12\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# epsilon=0 to use the policy without exploration\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     16\u001b[0m     img\u001b[38;5;241m.\u001b[39mset_data(env\u001b[38;5;241m.\u001b[39mrender(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m))  \u001b[38;5;66;03m# just update the data\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 209\u001b[0m, in \u001b[0;36mDQN.get_action\u001b[0;34m(self, state, epsilon)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_q\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[9], line 215\u001b[0m, in \u001b[0;36mDQN.get_q\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_q\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m    212\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03m    Compute Q function for a states\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    217\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_net\u001b[38;5;241m.\u001b[39mforward(state_tensor)  \u001b[38;5;66;03m# shape (1,  n_actions)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 7 at dim 1 (got 4)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "actions = env.action_space.n\n",
    "env.unwrapped.get_action_meanings()\n",
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    while not done:\n",
    "        time.sleep(0.1)\n",
    "        action = random.choice([0,1,2,3,4,5])\n",
    "        n_state, reward, done, info = env.step(action)       \n",
    "        score+=reward\n",
    "print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
